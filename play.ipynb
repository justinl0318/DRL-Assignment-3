{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym_super_mario_bros in /home/b10902078/.local/lib/python3.8/site-packages (7.4.0)\n",
      "Requirement already satisfied: nes-py>=8.1.4 in /home/b10902078/.local/lib/python3.8/site-packages (from gym_super_mario_bros) (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in /home/b10902078/.local/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (0.23.1)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /home/b10902078/.local/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (1.5.21)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /home/b10902078/.local/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (1.23.5)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /home/b10902078/.local/lib/python3.8/site-packages (from nes-py>=8.1.4->gym_super_mario_bros) (4.67.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /home/b10902078/.local/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (3.1.0)\n",
      "Requirement already satisfied: gym-notices>=0.0.4 in /home/b10902078/.local/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (0.0.8)\n",
      "Requirement already satisfied: importlib-metadata>=4.10.0; python_version < \"3.10\" in /home/b10902078/.local/lib/python3.8/site-packages (from gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (8.5.0)\n",
      "Requirement already satisfied: zipp>=3.20 in /home/b10902078/.local/lib/python3.8/site-packages (from importlib-metadata>=4.10.0; python_version < \"3.10\"->gym>=0.17.2->nes-py>=8.1.4->gym_super_mario_bros) (3.20.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wrappers.py\n",
    "import gym\n",
    "import cv2\n",
    "import numpy as np\n",
    "from gym.spaces import Box\n",
    "from collections import deque\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    \"\"\"Return every `skip`-th frame and repeat action during skip\"\"\"\n",
    "    def __init__(self, env, skip=4):\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "class GrayScaleObservation(gym.ObservationWrapper):\n",
    "    \"\"\"Convert frames to grayscale\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = self.observation_space.shape[:2]  # (height, width)\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # Convert RGB to grayscale\n",
    "        observation = cv2.cvtColor(observation, cv2.COLOR_RGB2GRAY)\n",
    "        return observation\n",
    "\n",
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    \"\"\"Resize observation frames to specified size\"\"\"\n",
    "    def __init__(self, env, size=84):\n",
    "        super().__init__(env)\n",
    "        if isinstance(size, int):\n",
    "            self.size = (size, size)\n",
    "        else:\n",
    "            self.size = tuple(size)\n",
    "            \n",
    "        obs_shape = self.size  # new shape (height, width)\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # Resize the observation\n",
    "        observation = cv2.resize(observation, self.size, interpolation=cv2.INTER_AREA)\n",
    "        return observation\n",
    "\n",
    "class NormalizeObservation(gym.ObservationWrapper):\n",
    "    \"\"\"Normalize observation values to range [0, 1]\"\"\"\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(low=0, high=1.0, \n",
    "                                    shape=self.observation_space.shape, \n",
    "                                    dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        # Normalize from [0, 255] to [0, 1]\n",
    "        return np.array(observation, dtype=np.float32) / 255.0\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    \"\"\"Stack n_frames last frames.\"\"\"\n",
    "    def __init__(self, env, n_frames=4):\n",
    "        super().__init__(env)\n",
    "        self.n_frames = n_frames\n",
    "        self.frames = deque([], maxlen=n_frames)\n",
    "        \n",
    "        # Update observation space to account for stacked frames\n",
    "        shp = env.observation_space.shape\n",
    "        obs_shape = (n_frames, *shp) if len(shp) == 2 else (n_frames, *shp[:-1])\n",
    "        \n",
    "        # Update observation space\n",
    "        low = np.min(env.observation_space.low)\n",
    "        high = np.max(env.observation_space.high)\n",
    "        self.observation_space = Box(\n",
    "            low=low, high=high, shape=obs_shape, dtype=env.observation_space.dtype\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        obs = self.env.reset()\n",
    "        for _ in range(self.n_frames):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_obs(), reward, done, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Stack frames along first dimension\n",
    "        return np.stack(self.frames, axis=0)\n",
    "    \n",
    "from IPython import embed\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torchrl.modules import NoisyLinear\n",
    "import numpy as np\n",
    "import random, os\n",
    "import pickle\n",
    "from collections import deque\n",
    "\n",
    "class DuelingDQN(nn.Module):\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DuelingDQN, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        # value stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        # self.value_stream = nn.Sequential(\n",
    "        #     NoisyLinear(conv_out_size, 512, std_init=2.5),\n",
    "        #     nn.ReLU(),\n",
    "        #     NoisyLinear(512, 1, std_init=2.5)\n",
    "        # )\n",
    "        \n",
    "        # advantage stream\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "        # self.advantage_stream = nn.Sequential(\n",
    "        #     NoisyLinear(conv_out_size, 512, std_init=2.5),\n",
    "        #     nn.ReLU(),\n",
    "        #     NoisyLinear(512, n_actions, std_init=2.5)\n",
    "        # )\n",
    "        \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape)) # 1 dummy input through conv\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size()[0]\n",
    "        conv_out = self.conv(x).view(batch_size, -1) # flatten \n",
    "        \n",
    "        value = self.value_stream(conv_out) # (B, 1)\n",
    "        advantage = self.advantage_stream(conv_out) # (B, n_actions)\n",
    "        \n",
    "        # combine value and advantage to get Q-values\n",
    "        # Q(s, a) = V(s) + (A(s, a) - mean(A(s, a')))\n",
    "        return value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "    \n",
    "class ICM(nn.Module):\n",
    "    def __init__(self, input_shape, embed_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.n_actions = n_actions\n",
    "        \n",
    "        self.conv_net = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            self.conv_net,\n",
    "            nn.Flatten(), # flatten will only flatten from dim 1 onwards (excluding the batch dimension)\n",
    "            nn.Linear(conv_out_size, embed_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # input: [phi(s_t), action (one-hot)] -> phi(s_t+1)\n",
    "        self.forward_model = nn.Sequential(\n",
    "            nn.Linear(embed_dim + n_actions, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, embed_dim)\n",
    "        )\n",
    "                \n",
    "        # input: [phi(s_t), phi(s_t+1)] -> action a\n",
    "        self.inverse_model = nn.Sequential(\n",
    "            nn.Linear(embed_dim * 2, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, n_actions)\n",
    "        )\n",
    "                \n",
    "    def forward(self, state, next_state, action):\n",
    "        phi = self.encoder(state) # (B, embed_dim)\n",
    "        phi_next = self.encoder(next_state) # (B, embed_dim)\n",
    "        action = action.squeeze(-1)  # (B, )\n",
    "        action_onehot = F.one_hot(action, self.n_actions).float() # (B, n_actions)\n",
    "        # detach to stop the gradients of the forward model from flowing into the encoder\n",
    "        fwd_model_input = torch.cat([phi.detach(), action_onehot], dim=1) # (B, embed_dim + n_actions); \n",
    "        inv_model_input = torch.cat([phi, phi_next], dim=1) # (B, embed_dim * 2)\n",
    "        predicted_phi_next = self.forward_model(fwd_model_input) # (B, embed_dim)\n",
    "        predicted_action = self.inverse_model(inv_model_input) # (B, n_actions)\n",
    "        return predicted_phi_next, predicted_action, phi_next\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv_net(torch.zeros(1, *shape)) # 1 dummy input through conv\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchrl.modules import NoisyLinear\n",
    "from torch.cuda.amp import autocast\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from collections import deque\n",
    "from tensordict import TensorDict\n",
    "from torchrl.data import LazyMemmapStorage, TensorDictPrioritizedReplayBuffer, TensorDictReplayBuffer\n",
    "from model import DuelingDQN, ICM\n",
    "import ipdb\n",
    "\n",
    "class MarioAgent:\n",
    "    def __init__(self, state_size=(4, 84, 84), action_size=12, batch_size=32, lr=2.5e-4, gamma=0.99, \n",
    "                 capacity=100000, update_target_freq=10000, tau=1.0, eps_start=1.0, eps_min=0.1, \n",
    "                 eps_fraction=500_000, alpha=0.6, beta=0.4, beta_increment=0.0001, num_envs=4, eps=1e-6):\n",
    "        \n",
    "        # Initialize parameters, networks, optimizer, replay buffer, etc.\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # hyperparameters\n",
    "        self.gamma = gamma  # discount factor\n",
    "        self.epsilon = eps_start  # exploration rate\n",
    "        self.epsilon_start = eps_start\n",
    "        self.epsilon_min = eps_min\n",
    "        self.epsilon_fraction = eps_fraction\n",
    "        self.learning_rate = lr\n",
    "        self.update_target_freq = update_target_freq\n",
    "        self.batch_size = batch_size\n",
    "        self.capacity = capacity\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.beta_increment = beta_increment\n",
    "        self.eps = eps\n",
    "        self.burn_in = 5000 # min. experiences before training\n",
    "        \n",
    "        # Neural Networks - using Dueling architecture\n",
    "        self.policy_net = DuelingDQN(state_size, action_size).to(self.device)\n",
    "        self.target_net = DuelingDQN(state_size, action_size).to(self.device)\n",
    "        # self.policy_net = torch.compile(self.policy_net)\n",
    "        # self.target_net = torch.compile(self.target_net)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "        \n",
    "        # ICM\n",
    "        self.icm = ICM(state_size, embed_dim=512, n_actions=action_size).to(self.device)\n",
    "        self.icm_optimizer = torch.optim.Adam(self.icm.parameters(), lr=self.learning_rate)\n",
    "        # scale of intrinsic reward\n",
    "        self.icm_beta = 0.01\n",
    "        # losses weighting\n",
    "        self.icm_lambda_fwd = 0.8\n",
    "        self.icm_lambda_inv = 0.2\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)\n",
    "        \n",
    "        # PrioritizedReplayBuffer from torchrl\n",
    "        scratch_dir = \"/data1/b10902078/replay_buffer\"\n",
    "        # scratch_dir = \"./replay_buffer\"\n",
    "        storage = LazyMemmapStorage(max_size=self.capacity, scratch_dir=scratch_dir, device=torch.device(\"cpu\"))\n",
    "        # self.memory = TensorDictPrioritizedReplayBuffer(storage=storage, batch_size=batch_size, alpha=self.alpha, beta=self.beta, eps=eps, priority_key=\"td_error\")\n",
    "        self.memory = TensorDictReplayBuffer(storage=storage, batch_size=batch_size)\n",
    "        \n",
    "        # For updating target network\n",
    "        self.learn_count = 0\n",
    "        self.total_steps = 0\n",
    "        \n",
    "    def act(self, state, deterministic=False):\n",
    "        \"\"\"Select action using epsilon-greedy policy\"\"\"\n",
    "        # Convert state to torch tensor if it's not already\n",
    "        if isinstance(state, np.ndarray):\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device) # create batch dimension, (1, stack_size, 84, 84)\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if not deterministic and random.random() < self.epsilon: # exploration\n",
    "            return random.randrange(self.action_size)\n",
    "        else: # exploitation\n",
    "            # for m in self.policy_net.modules():\n",
    "            #     if isinstance(m, NoisyLinear):\n",
    "            #         m.reset_noise()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                # with torch.autocast(device_type=self.device.type, dtype=torch.bfloat16):\n",
    "                q_values = self.policy_net(state)\n",
    "            return q_values.argmax(dim=1).item()\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Train the network with a batch from replay memory\"\"\"\n",
    "        # Check if buffer has enough samples\n",
    "        # if self.total_steps < self.burn_in or len(self.memory) < self.batch_size:\n",
    "        #     return\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch from replay buffer\n",
    "        td_batch, info = self.memory.sample(self.batch_size, return_info=True)\n",
    "        states = td_batch[\"state\"].to(self.device) # (B, 4, 84, 84)\n",
    "        actions = td_batch[\"action\"].long().to(self.device) # (B, 1)\n",
    "        rewards = td_batch[\"reward\"].to(self.device) # (B, 1)\n",
    "        next_states = td_batch[\"next_state\"].to(self.device) # (B, 4, 84, 84)\n",
    "        dones = td_batch[\"done\"].to(self.device) # (B, 1)\n",
    "        # indices = td_batch[\"index\"] # (B,)\n",
    "        # weights = td_batch[\"_weight\"].to(self.device) # (B,)\n",
    "        \n",
    "        # —— compute ICM losses & intrinsic reward —— \n",
    "        pred_phi_next, pred_action_logits, phi_next = self.icm(states, next_states, actions)\n",
    "        # forward loss per sample\n",
    "        fwd_loss_sample = F.mse_loss(pred_phi_next, phi_next.detach(), reduction='none').mean(dim=1, keepdim=True) # (B, 1)\n",
    "        # inverse loss per sample\n",
    "        inv_loss_sample = F.cross_entropy(pred_action_logits, actions.squeeze(-1), reduction='none').unsqueeze(1) # (B, 1)\n",
    "        # intrinsic reward signal\n",
    "        intrinsic_reward = self.icm_beta * fwd_loss_sample # (B, 1)\n",
    "        # augment external reward\n",
    "        rewards = rewards + intrinsic_reward # (B, 1)\n",
    "        # total ICM loss\n",
    "        icm_loss = self.icm_lambda_fwd * fwd_loss_sample.mean() + self.icm_lambda_inv * inv_loss_sample.mean()\n",
    "        \n",
    "        # ipdb.set_trace()\n",
    "        # for net in [self.policy_net, self.target_net]:\n",
    "        #     for m in net.modules():\n",
    "        #         if isinstance(m, NoisyLinear):\n",
    "        #             m.reset_noise()\n",
    "        \n",
    "        # Compute current Q values\n",
    "        q_values = self.policy_net(states).gather(1, actions)  # (batch_size, 1)\n",
    "        \n",
    "        # Double DQN: use online network to select action and target network to evaluate it\n",
    "        with torch.no_grad():\n",
    "            # with torch.autocast(device_type=self.device.type, dtype=torch.bfloat16):\n",
    "            # Select actions using the online policy network\n",
    "            best_actions = self.policy_net(next_states).argmax(1, keepdim=True)  # (batch_size, 1)\n",
    "            # Evaluate those actions using the target network\n",
    "            next_q_values = self.target_net(next_states).gather(1, best_actions)  # (batch_size, 1)\n",
    "        \n",
    "        # Compute expected Q values\n",
    "        expected_q_values = rewards + self.gamma * next_q_values * (1.0 - dones.float())  # (batch_size, 1)\n",
    "\n",
    "        # # Calculate TD errors for updating priorities\n",
    "        # td_errors = (q_values - expected_q_values).abs().detach().cpu().numpy().flatten() + self.eps # (B,)\n",
    "\n",
    "        # # Update priorities in buffer\n",
    "        # self.memory.update_priority(indices, td_errors)\n",
    "\n",
    "        # # Apply importance sampling weights\n",
    "        # weights = weights.unsqueeze(1)  # (batch_size, 1)\n",
    "        \n",
    "        # # Calculate loss using Huber loss (smooth L1)\n",
    "        # loss = (weights * F.smooth_l1_loss(q_values, expected_q_values, reduction=\"none\")).mean()\n",
    "        \n",
    "        # loss = F.smooth_l1_loss(q_values, expected_q_values)\n",
    "        q_loss = F.smooth_l1_loss(q_values, expected_q_values)\n",
    "        total_loss = q_loss + icm_loss\n",
    "\n",
    "        # Gradient descent\n",
    "        self.optimizer.zero_grad()\n",
    "        self.icm_optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 10.0)  # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.icm.parameters(), 10.0)\n",
    "        self.optimizer.step()\n",
    "        self.icm_optimizer.step()\n",
    "        \n",
    "        # Update target network periodically\n",
    "        self.learn_count += 1\n",
    "        if self.learn_count % self.update_target_freq == 0:\n",
    "            self.update_target()\n",
    "            \n",
    "    def update_target(self):\n",
    "        \"\"\"Update target network with policy network weights\"\"\"\n",
    "        if self.tau < 1.0:\n",
    "            # Soft update\n",
    "            for target_param, policy_param in zip(self.target_net.parameters(), self.policy_net.parameters()):\n",
    "                target_param.data.copy_(self.tau * policy_param.data + (1.0 - self.tau) * target_param.data)\n",
    "        else:\n",
    "            # Hard update\n",
    "            self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.epsilon_start * 10 ** (-self.total_steps / self.epsilon_fraction), self.epsilon_min)\n",
    "    \n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Store transition in replay buffer\"\"\"\n",
    "        td = TensorDict({\n",
    "            \"state\": torch.tensor(state, device=\"cpu\"),\n",
    "            \"action\": torch.tensor([action], device=\"cpu\"),\n",
    "            \"reward\": torch.tensor([reward], device=\"cpu\"),\n",
    "            \"next_state\": torch.tensor(next_state, device=\"cpu\"),\n",
    "            \"done\": torch.tensor([done], device=\"cpu\")\n",
    "        })\n",
    "        self.memory.add(td)\n",
    "        self.total_steps += 1\n",
    "            \n",
    "    def save(self, path):\n",
    "        \"\"\"Save model to disk\"\"\"\n",
    "        torch.save({\n",
    "            'policy_net': self.policy_net.state_dict(),\n",
    "            'target_net': self.target_net.state_dict(),\n",
    "            'optimizer': self.optimizer.state_dict(),\n",
    "            'epsilon': self.epsilon,\n",
    "            'learn_count': self.learn_count,\n",
    "            'total_steps': self.total_steps,\n",
    "            'icm': self.icm.state_dict(),           # Add ICM model state\n",
    "            'icm_optimizer': self.icm_optimizer.state_dict()  # Add ICM optimizer state\n",
    "        }, path)\n",
    "\n",
    "    def load(self, path):\n",
    "        \"\"\"Load model from disk\"\"\"\n",
    "        checkpoint = torch.load(path, map_location=self.device)\n",
    "        self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "        self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "        self.optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        self.epsilon = checkpoint['epsilon']\n",
    "        self.learn_count = checkpoint['learn_count']\n",
    "        self.total_steps = checkpoint['total_steps']\n",
    "        self.icm.load_state_dict(checkpoint['icm'])           # Load ICM model state\n",
    "        self.icm_optimizer.load_state_dict(checkpoint['icm_optimizer'])  # Load ICM optimizer state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import gym\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from mario_agent import MarioAgent\n",
    "from wrappers import SkipFrame, GrayScaleObservation, ResizeObservation, NormalizeObservation, FrameStack\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/b10902078/.local/lib/python3.8/site-packages/gym/envs/registration.py:505: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3` with the environment ID `SuperMarioBros-v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/b10902078/DRL/hw3/DRL-Assignment-3/mario_agent.py:213: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/20: Reward = 7565.0\n",
      "Episode 2/20: Reward = 6182.0\n",
      "Episode 3/20: Reward = 6182.0\n",
      "Episode 4/20: Reward = 7565.0\n",
      "Episode 5/20: Reward = 6473.0\n",
      "Episode 6/20: Reward = 6600.0\n",
      "Episode 7/20: Reward = 7565.0\n",
      "Episode 8/20: Reward = 6182.0\n",
      "Episode 9/20: Reward = 6600.0\n",
      "Episode 10/20: Reward = 7904.0\n",
      "Episode 11/20: Reward = 7565.0\n",
      "Episode 12/20: Reward = 6473.0\n",
      "Episode 13/20: Reward = 7565.0\n",
      "Episode 14/20: Reward = 7899.0\n",
      "Episode 15/20: Reward = 6600.0\n",
      "Episode 16/20: Reward = 7899.0\n",
      "Episode 17/20: Reward = 6182.0\n",
      "Episode 18/20: Reward = 7899.0\n",
      "Episode 19/20: Reward = 7565.0\n",
      "Episode 20/20: Reward = 7565.0\n",
      "\n",
      "Average Reward: 7101.50\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0')\n",
    "env = JoypadSpace(env, COMPLEX_MOVEMENT)\n",
    "\n",
    "# Apply preprocessing wrappers (same as training)\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env)\n",
    "env = ResizeObservation(env, size=84)\n",
    "env = NormalizeObservation(env)\n",
    "env = FrameStack(env, n_frames=4)\n",
    "\n",
    "# State shape should match the environment's observation space\n",
    "state_shape = env.observation_space.shape\n",
    "action_size = env.action_space.n\n",
    "\n",
    "# Create agent\n",
    "agent = MarioAgent(state_size=state_shape, action_size=action_size)\n",
    "\n",
    "# Load trained model\n",
    "model_path = \"models/mario_dqn_ep8500.pth\"\n",
    "agent.load(model_path)\n",
    "agent.epsilon = 0.1  # No exploration during evaluation\n",
    "\n",
    "total_rewards = []\n",
    "episodes = 100\n",
    "\n",
    "# Evaluation loop\n",
    "for episode in range(1, episodes + 1):\n",
    "    state = env.reset()\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    first_step_random = True\n",
    "    \n",
    "    # Episode loop\n",
    "    while not done:\n",
    "        # frame = env.render(mode='rgb_array')\n",
    "        # clear_output(wait=True)\n",
    "        # plt.imshow(frame)\n",
    "        # plt.axis('off')\n",
    "        # plt.show()\n",
    "        if first_step_random:\n",
    "            action = random.randint(0, env.action_space.n - 1)\n",
    "            first_step_random = False\n",
    "        else:\n",
    "            # Select action (no exploration)\n",
    "            action = agent.act(state, deterministic=True)\n",
    "        \n",
    "        # Take action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update state and tracking info\n",
    "        state = next_state\n",
    "        episode_reward += reward\n",
    "    \n",
    "    # Track metrics\n",
    "    total_rewards.append(episode_reward)\n",
    "    print(f\"Episode {episode}/{episodes}: Reward = {episode_reward}\")\n",
    "\n",
    "# Summary\n",
    "avg_reward = np.mean(total_rewards)\n",
    "# plt.imshow(frame)\n",
    "print(f\"\\nAverage Reward: {avg_reward:.2f}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3136])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(32, 64, 7, 7)   # (batch, channels, height, width)\n",
    "flatten = nn.Flatten()\n",
    "out = flatten(x)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor([[ 0.0041,  0.0017, -0.0068, -0.0446, -0.0543],\n",
      "        [-0.0109, -0.0046,  0.0185,  0.1210,  0.1472],\n",
      "        [-0.0955,  0.0279, -0.0008, -0.0244,  0.0438],\n",
      "        [-0.0370,  0.0157, -0.0118, -0.0863, -0.0713],\n",
      "        [-0.0560,  0.0147,  0.0036,  0.0126,  0.0565]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Dummy encoder and forward model\n",
    "encoder = nn.Linear(10, 5)    # encoder: input 10 -> feature 5\n",
    "forward_model = nn.Linear(5, 5)  # forward_model: input 5 -> predict 5\n",
    "\n",
    "# Input\n",
    "state = torch.randn(2, 10)   # batch of 2\n",
    "next_state = torch.randn(2, 10)\n",
    "\n",
    "# Get features\n",
    "phi = encoder(state)\n",
    "phi_next = encoder(next_state)\n",
    "\n",
    "# Predict next feature\n",
    "pred_phi_next = forward_model(phi.detach())\n",
    "\n",
    "# Loss: MSE between pred_phi_next and detached phi_next\n",
    "loss = F.mse_loss(pred_phi_next, phi_next.detach())\n",
    "loss.backward()\n",
    "\n",
    "print(encoder.weight.grad)   # <= look here\n",
    "print(forward_model.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "pred_phi_next = torch.randn(32, 256)\n",
    "phi_next = torch.randn(32, 256)\n",
    "fwd_loss_sample = F.mse_loss(pred_phi_next, phi_next.detach(), reduction='none').mean(dim=1, keepdim=True)\n",
    "fwd_loss_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_action_logits = torch.randn(32, 12)\n",
    "actions = torch.randint(0, 12, (32,))\n",
    "print(actions.shape)\n",
    "F.cross_entropy(pred_action_logits, actions, reduction='none').unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.arange(5, dtype=torch.float).view(5, 1)\n",
    "print(test.shape)\n",
    "test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([0, 1, 2])     # shape (3,) → OK\n",
    "F.one_hot(x, num_classes=4).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[0], [1], [2]])  # shape (3,1) → ❌ error\n",
    "print(x.shape)\n",
    "F.one_hot(x, num_classes=4).shape  # fix it by .squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.001, 0.01)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1e-3, 1e-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
